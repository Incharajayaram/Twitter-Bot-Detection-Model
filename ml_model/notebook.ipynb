{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading label data...\n",
      "Loading edge data...\n",
      "Processing edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing edge chunks: 171it [16:16,  5.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Data Object: Data(edge_index=[2, 3745300], y=[1000000])\n",
      "Number of nodes: 996093\n",
      "Number of edges: 3745300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Twitter-Bot-Detection-Model\\venv\\lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'y', 'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load label (node) data\n",
    "print(\"Loading label data...\")\n",
    "label_file = 'D:/Twitter-Bot-Detection-Model/ml_model/dataset/label.csv'\n",
    "labels_df = pd.read_csv(label_file, dtype={'id': 'object', 'label': 'category'})\n",
    "\n",
    "# Convert user IDs to integers more efficiently\n",
    "labels_df['id'] = pd.to_numeric(labels_df['id'].str[1:], errors='coerce')\n",
    "\n",
    "# Create a mapping from old ID to new index using a dictionary comprehension\n",
    "id_to_index = {old_id: new_index for new_index, old_id in enumerate(labels_df['id'])}\n",
    "\n",
    "# Map bot/human labels to binary (bot = 0, human = 1)\n",
    "label_map = {'bot': 0, 'human': 1}\n",
    "labels_df['label'] = labels_df['label'].map(label_map)\n",
    "\n",
    "# Create node labels tensor\n",
    "num_nodes = len(id_to_index)\n",
    "y = torch.zeros(num_nodes, dtype=torch.long)\n",
    "y[labels_df['id'].map(id_to_index)] = torch.tensor(labels_df['label'].values, dtype=torch.long)\n",
    "\n",
    "# Load edge data using pandas for initial processing\n",
    "print(\"Loading edge data...\")\n",
    "edge_file = 'D:/Twitter-Bot-Detection-Model/ml_model/dataset/edge.csv'\n",
    "chunks = pd.read_csv(edge_file, dtype={'source_id': 'object', 'target_id': 'object'}, chunksize=1000000)\n",
    "\n",
    "# Process edges\n",
    "print(\"Processing edges...\")\n",
    "edge_list = []\n",
    "for chunk in tqdm(chunks, desc=\"Processing edge chunks\"):\n",
    "    chunk['source_id'] = pd.to_numeric(chunk['source_id'].str[1:], errors='coerce')\n",
    "    chunk['target_id'] = pd.to_numeric(chunk['target_id'].str[1:], errors='coerce')\n",
    "    \n",
    "    # Filter out rows with NaN values\n",
    "    chunk = chunk.dropna()\n",
    "    \n",
    "    source_indices = chunk['source_id'].map(id_to_index)\n",
    "    target_indices = chunk['target_id'].map(id_to_index)\n",
    "    \n",
    "    # Filter out edges that don't have corresponding nodes\n",
    "    mask = source_indices.notna() & target_indices.notna()\n",
    "    source_indices = source_indices[mask].astype(int)\n",
    "    target_indices = target_indices[mask].astype(int)\n",
    "    \n",
    "    edge_list.append(np.array([source_indices, target_indices]))\n",
    "\n",
    "# Combine edge parts\n",
    "edge_index = np.concatenate(edge_list, axis=1)\n",
    "\n",
    "# Convert to torch tensor\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "# Create the PyTorch-Geometric data object\n",
    "data = Data(edge_index=edge_index, y=y)\n",
    "\n",
    "print(f\"Graph Data Object: {data}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting edges and nodes...\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "def efficient_node_edge_inspection(edge_file, label_file, total_edges, chunk_size=1_000_000, num_processes=mp.cpu_count()):\n",
    "    print(\"\\nInspecting edges and nodes...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load all labeled nodes\n",
    "    labeled_nodes = set()\n",
    "    with open(label_file, 'r', newline='') as labelfile:\n",
    "        label_reader = csv.reader(labelfile)\n",
    "        next(label_reader)  # Skip header\n",
    "        for row in label_reader:\n",
    "            labeled_nodes.add(row[0])  # Assuming the node ID is in the first column\n",
    "\n",
    "    with mp.Manager() as manager:\n",
    "        shared_degree_count = manager.dict()\n",
    "        shared_nodes = manager.dict()\n",
    "        \n",
    "        chunks = list(chunk_generator(edge_file, chunk_size, total_edges))\n",
    "        \n",
    "        with mp.Pool(processes=num_processes) as pool:\n",
    "            results = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                result = pool.apply_async(process_chunk, (chunk, i))\n",
    "                results.append(result)\n",
    "            \n",
    "            processed_edges = 0\n",
    "            for result in results:\n",
    "                local_degree_count, local_nodes, chunk_size = result.get()\n",
    "                \n",
    "                # Update shared data structures\n",
    "                for node, count in local_degree_count.items():\n",
    "                    if node in shared_degree_count:\n",
    "                        shared_degree_count[node] += count\n",
    "                    else:\n",
    "                        shared_degree_count[node] = count\n",
    "                \n",
    "                for node in local_nodes:\n",
    "                    shared_nodes[node] = True\n",
    "                \n",
    "                processed_edges += chunk_size\n",
    "                print(f\"Processed approximately {processed_edges} edges. Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        degree_count = Counter(shared_degree_count)\n",
    "        all_nodes = set(shared_nodes.keys())\n",
    "\n",
    "    unique_nodes = len(all_nodes)\n",
    "    \n",
    "    # Identify missing nodes\n",
    "    missing_nodes = all_nodes - labeled_nodes\n",
    "    extra_labeled_nodes = labeled_nodes - all_nodes\n",
    "    \n",
    "    print(f\"\\nTotal number of edges processed: {processed_edges}\")\n",
    "    print(f\"Number of unique nodes in edge file: {unique_nodes}\")\n",
    "    print(f\"Number of labeled nodes: {len(labeled_nodes)}\")\n",
    "    print(f\"Number of nodes in edge file but not in label file: {len(missing_nodes)}\")\n",
    "    print(f\"Number of nodes in label file but not in edge file: {len(extra_labeled_nodes)}\")\n",
    "\n",
    "    degrees = list(degree_count.values())\n",
    "    print(f\"\\nDegree statistics:\")\n",
    "    print(f\"Average degree: {sum(degrees) / len(degrees):.2f}\")\n",
    "    print(f\"Maximum degree: {max(degrees)}\")\n",
    "    print(f\"Minimum degree: {min(degrees)}\")\n",
    "\n",
    "    print(f\"\\nTotal time taken: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return degree_count, missing_nodes, extra_labeled_nodes\n",
    "\n",
    "def chunk_generator(file_path, chunk_size, total_edges):\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip header\n",
    "        chunk = []\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= total_edges:\n",
    "                break\n",
    "            chunk.append(row)\n",
    "            if len(chunk) == chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "def process_chunk(chunk, chunk_id):\n",
    "    local_degree_count = Counter()\n",
    "    local_nodes = set()\n",
    "\n",
    "    for source, target in chunk:\n",
    "        local_degree_count[source] += 1\n",
    "        local_degree_count[target] += 1\n",
    "        local_nodes.add(source)\n",
    "        local_nodes.add(target)\n",
    "\n",
    "    return local_degree_count, local_nodes, len(chunk)\n",
    "\n",
    "# Usage\n",
    "edge_file = 'D:/Twitter-Bot-Detection-Model/ml_model/dataset/edge.csv'\n",
    "label_file = 'D:/Twitter-Bot-Detection-Model/ml_model/dataset/label.csv'\n",
    "total_edges = 3745300  # The actual number of edges in your file\n",
    "\n",
    "# Run the node-aware edge inspection\n",
    "degree_count, missing_nodes, extra_labeled_nodes = efficient_node_edge_inspection(edge_file, label_file, total_edges)\n",
    "\n",
    "# You can now use degree_count, unique_edges, missing_nodes, and extra_labeled_nodes for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix shape: torch.Size([1000000, 1000000])\n",
      "Number of non-zero entries: 3334408\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import coalesce, to_scipy_sparse_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have already run the previous loading code\n",
    "\n",
    "# Coalesce the edge_index to remove duplicate edges and ensure undirected graph\n",
    "edge_index, _ = coalesce(edge_index, None, num_nodes, num_nodes)\n",
    "\n",
    "# Convert edge_index to a PyTorch sparse matrix\n",
    "# Create the adjacency matrix as a sparse COO tensor\n",
    "adj_sparse = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (num_nodes, num_nodes))\n",
    "\n",
    "# Convert the adjacency matrix to a scipy sparse matrix (optional, if needed for further processing)\n",
    "adj_scipy = to_scipy_sparse_matrix(edge_index)\n",
    "\n",
    "# Print the sparsity of the matrix\n",
    "print(f\"Sparse matrix shape: {adj_sparse.shape}\")\n",
    "print(f\"Number of non-zero entries: {adj_sparse._nnz()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "# Calculate degree centrality\n",
    "degree_centrality = degree(data.edge_index[0], num_nodes=data.num_nodes).float()  # Out-degree\n",
    "\n",
    "# Initialize a tensor for features with zeros\n",
    "num_features = 10\n",
    "x = torch.zeros((num_nodes, num_features))  # Use zeros for all nodes\n",
    "\n",
    "# Create a full degree centrality tensor\n",
    "full_degree_centrality = torch.zeros(num_nodes)\n",
    "full_degree_centrality[:degree_centrality.size(0)] = degree_centrality  # Fill in degree centrality values\n",
    "\n",
    "# Assign degree centrality to the first feature for all nodes\n",
    "x[:, 0] = full_degree_centrality  # Assign full degree centrality values\n",
    "\n",
    "# Generate random features for the other dimensions (1 to num_features-1)\n",
    "x[:, 1:] = torch.rand(num_nodes, num_features - 1)  # Random features for demonstration\n",
    "\n",
    "# Create a new Data object with the synthetic features\n",
    "data.x = x  # Adding the synthetic features to the Data object\n",
    "\n",
    "print(data.x.shape)  # Check the shape of the feature tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[1000000, 10], edge_index=[2, 3334408], y=[1000000])\n",
      "Number of nodes: 1000000\n",
      "Number of edges: 3334408\n",
      "Feature tensor shape: torch.Size([1000000, 10])\n",
      "Edge index shape: torch.Size([2, 3334408])\n",
      "Labels tensor shape: torch.Size([1000000])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create the Data object with features, edge index, and labels\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Print the Data object to verify its structure\n",
    "print(f\"Data object: {data}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Feature tensor shape: {data.x.shape}\")\n",
    "print(f\"Edge index shape: {data.edge_index.shape}\")\n",
    "print(f\"Labels tensor shape: {data.y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mask: 800000 nodes in training set.\n",
      "Validation mask: 100000 nodes in validation set.\n",
      "Test mask: 100000 nodes in test set.\n"
     ]
    }
   ],
   "source": [
    "# Example split (80% train, 10% validation, 10% test)\n",
    "num_train = int(0.8 * num_nodes)\n",
    "num_val = int(0.1 * num_nodes)\n",
    "\n",
    "# Create boolean masks for train, validation, and test splits\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[:num_train] = True\n",
    "\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask[num_train:num_train + num_val] = True\n",
    "\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask[num_train + num_val:] = True\n",
    "\n",
    "# Assign masks to the data object\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Print the masks to verify\n",
    "print(\"Training mask:\", data.train_mask.sum().item(), \"nodes in training set.\")\n",
    "print(\"Validation mask:\", data.val_mask.sum().item(), \"nodes in validation set.\")\n",
    "print(\"Test mask:\", data.test_mask.sum().item(), \"nodes in test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Create the model\n",
    "model = GCN(num_features=num_features, num_classes=2)  # Adjust num_classes as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 95.6816\n",
      "Epoch 2: Loss: 85.3128\n",
      "Epoch 3: Loss: 74.1763\n",
      "Epoch 4: Loss: 64.7125\n",
      "Epoch 5: Loss: 55.0502\n",
      "Epoch 6: Loss: 45.9568\n",
      "Epoch 7: Loss: 36.7742\n",
      "Epoch 8: Loss: 28.4627\n",
      "Epoch 9: Loss: 20.8869\n",
      "Epoch 10: Loss: 14.1611\n",
      "Epoch 11: Loss: 9.9347\n",
      "Epoch 12: Loss: 6.8703\n",
      "Epoch 13: Loss: 5.7702\n",
      "Epoch 14: Loss: 5.1379\n",
      "Epoch 15: Loss: 5.0403\n",
      "Epoch 16: Loss: 5.0848\n",
      "Epoch 17: Loss: 5.4152\n",
      "Epoch 18: Loss: 5.5252\n",
      "Epoch 19: Loss: 5.7926\n",
      "Epoch 20: Loss: 6.0269\n",
      "Epoch 21: Loss: 6.2700\n",
      "Epoch 22: Loss: 6.3025\n",
      "Epoch 23: Loss: 6.5503\n",
      "Epoch 24: Loss: 6.6611\n",
      "Epoch 25: Loss: 6.8470\n",
      "Epoch 26: Loss: 6.8154\n",
      "Epoch 27: Loss: 6.8376\n",
      "Epoch 28: Loss: 6.7988\n",
      "Epoch 29: Loss: 6.8834\n",
      "Epoch 30: Loss: 6.8273\n",
      "Epoch 31: Loss: 6.7099\n",
      "Epoch 32: Loss: 6.5625\n",
      "Epoch 33: Loss: 6.4601\n",
      "Epoch 34: Loss: 6.4499\n",
      "Epoch 35: Loss: 6.2420\n",
      "Epoch 36: Loss: 6.0854\n",
      "Epoch 37: Loss: 6.0031\n",
      "Epoch 38: Loss: 5.8664\n",
      "Epoch 39: Loss: 5.7423\n",
      "Epoch 40: Loss: 5.5997\n",
      "Epoch 41: Loss: 5.4771\n",
      "Epoch 42: Loss: 5.2865\n",
      "Epoch 43: Loss: 5.1814\n",
      "Epoch 44: Loss: 5.0374\n",
      "Epoch 45: Loss: 4.8618\n",
      "Epoch 46: Loss: 4.6690\n",
      "Epoch 47: Loss: 4.5874\n",
      "Epoch 48: Loss: 4.4281\n",
      "Epoch 49: Loss: 4.3296\n",
      "Epoch 50: Loss: 4.1867\n",
      "Epoch 51: Loss: 4.1261\n",
      "Epoch 52: Loss: 3.9924\n",
      "Epoch 53: Loss: 3.8139\n",
      "Epoch 54: Loss: 3.7324\n",
      "Epoch 55: Loss: 3.6493\n",
      "Epoch 56: Loss: 3.5794\n",
      "Epoch 57: Loss: 3.5102\n",
      "Epoch 58: Loss: 3.4206\n",
      "Epoch 59: Loss: 3.3495\n",
      "Epoch 60: Loss: 3.2210\n",
      "Epoch 61: Loss: 3.1909\n",
      "Epoch 62: Loss: 3.1770\n",
      "Epoch 63: Loss: 3.0390\n",
      "Epoch 64: Loss: 2.9728\n",
      "Epoch 65: Loss: 2.9137\n",
      "Epoch 66: Loss: 2.8687\n",
      "Epoch 67: Loss: 2.8082\n",
      "Epoch 68: Loss: 2.7855\n",
      "Epoch 69: Loss: 2.7193\n",
      "Epoch 70: Loss: 2.6893\n",
      "Epoch 71: Loss: 2.6061\n",
      "Epoch 72: Loss: 2.6056\n",
      "Epoch 73: Loss: 2.5345\n",
      "Epoch 74: Loss: 2.4998\n",
      "Epoch 75: Loss: 2.4771\n",
      "Epoch 76: Loss: 2.4040\n",
      "Epoch 77: Loss: 2.3450\n",
      "Epoch 78: Loss: 2.2877\n",
      "Epoch 79: Loss: 2.2166\n",
      "Epoch 80: Loss: 2.1992\n",
      "Epoch 81: Loss: 2.1506\n",
      "Epoch 82: Loss: 2.1282\n",
      "Epoch 83: Loss: 2.0735\n",
      "Epoch 84: Loss: 2.0286\n",
      "Epoch 85: Loss: 1.9654\n",
      "Epoch 86: Loss: 1.9067\n",
      "Epoch 87: Loss: 1.8941\n",
      "Epoch 88: Loss: 1.8543\n",
      "Epoch 89: Loss: 1.8104\n",
      "Epoch 90: Loss: 1.8226\n",
      "Epoch 91: Loss: 1.7663\n",
      "Epoch 92: Loss: 1.7044\n",
      "Epoch 93: Loss: 1.6459\n",
      "Epoch 94: Loss: 1.6402\n",
      "Epoch 95: Loss: 1.5928\n",
      "Epoch 96: Loss: 1.5750\n",
      "Epoch 97: Loss: 1.5557\n",
      "Epoch 98: Loss: 1.5093\n",
      "Epoch 99: Loss: 1.4990\n",
      "Epoch 100: Loss: 1.4442\n",
      "Epoch 101: Loss: 1.4015\n",
      "Epoch 102: Loss: 1.3885\n",
      "Epoch 103: Loss: 1.3695\n",
      "Epoch 104: Loss: 1.3407\n",
      "Epoch 105: Loss: 1.3137\n",
      "Epoch 106: Loss: 1.3077\n",
      "Epoch 107: Loss: 1.2720\n",
      "Epoch 108: Loss: 1.2373\n",
      "Epoch 109: Loss: 1.2068\n",
      "Epoch 110: Loss: 1.1800\n",
      "Epoch 111: Loss: 1.1507\n",
      "Epoch 112: Loss: 1.1454\n",
      "Epoch 113: Loss: 1.1129\n",
      "Epoch 114: Loss: 1.0922\n",
      "Epoch 115: Loss: 1.0663\n",
      "Epoch 116: Loss: 1.0337\n",
      "Epoch 117: Loss: 1.0457\n",
      "Epoch 118: Loss: 1.0100\n",
      "Epoch 119: Loss: 0.9849\n",
      "Epoch 120: Loss: 0.9868\n",
      "Epoch 121: Loss: 0.9547\n",
      "Epoch 122: Loss: 0.9390\n",
      "Epoch 123: Loss: 0.9208\n",
      "Epoch 124: Loss: 0.8854\n",
      "Epoch 125: Loss: 0.9024\n",
      "Epoch 126: Loss: 0.8807\n",
      "Epoch 127: Loss: 0.8668\n",
      "Epoch 128: Loss: 0.8354\n",
      "Epoch 129: Loss: 0.8350\n",
      "Epoch 130: Loss: 0.8086\n",
      "Epoch 131: Loss: 0.8014\n",
      "Epoch 132: Loss: 0.7760\n",
      "Epoch 133: Loss: 0.7745\n",
      "Epoch 134: Loss: 0.7610\n",
      "Epoch 135: Loss: 0.7432\n",
      "Epoch 136: Loss: 0.7309\n",
      "Epoch 137: Loss: 0.7113\n",
      "Epoch 138: Loss: 0.6980\n",
      "Epoch 139: Loss: 0.6911\n",
      "Epoch 140: Loss: 0.6765\n",
      "Epoch 141: Loss: 0.6727\n",
      "Epoch 142: Loss: 0.6553\n",
      "Epoch 143: Loss: 0.6470\n",
      "Epoch 144: Loss: 0.6409\n",
      "Epoch 145: Loss: 0.6342\n",
      "Epoch 146: Loss: 0.6182\n",
      "Epoch 147: Loss: 0.6090\n",
      "Epoch 148: Loss: 0.5941\n",
      "Epoch 149: Loss: 0.5962\n",
      "Epoch 150: Loss: 0.5838\n",
      "Epoch 151: Loss: 0.5764\n",
      "Epoch 152: Loss: 0.5724\n",
      "Epoch 153: Loss: 0.5599\n",
      "Epoch 154: Loss: 0.5517\n",
      "Epoch 155: Loss: 0.5470\n",
      "Epoch 156: Loss: 0.5322\n",
      "Epoch 157: Loss: 0.5348\n",
      "Epoch 158: Loss: 0.5278\n",
      "Epoch 159: Loss: 0.5163\n",
      "Epoch 160: Loss: 0.5189\n",
      "Epoch 161: Loss: 0.5090\n",
      "Epoch 162: Loss: 0.5035\n",
      "Epoch 163: Loss: 0.4982\n",
      "Epoch 164: Loss: 0.4951\n",
      "Epoch 165: Loss: 0.4845\n",
      "Epoch 166: Loss: 0.4787\n",
      "Epoch 167: Loss: 0.4814\n",
      "Epoch 168: Loss: 0.4757\n",
      "Epoch 169: Loss: 0.4668\n",
      "Epoch 170: Loss: 0.4663\n",
      "Epoch 171: Loss: 0.4608\n",
      "Epoch 172: Loss: 0.4601\n",
      "Epoch 173: Loss: 0.4554\n",
      "Epoch 174: Loss: 0.4528\n",
      "Epoch 175: Loss: 0.4490\n",
      "Epoch 176: Loss: 0.4465\n",
      "Epoch 177: Loss: 0.4432\n",
      "Epoch 178: Loss: 0.4430\n",
      "Epoch 179: Loss: 0.4335\n",
      "Epoch 180: Loss: 0.4327\n",
      "Epoch 181: Loss: 0.4314\n",
      "Epoch 182: Loss: 0.4308\n",
      "Epoch 183: Loss: 0.4263\n",
      "Epoch 184: Loss: 0.4245\n",
      "Epoch 185: Loss: 0.4206\n",
      "Epoch 186: Loss: 0.4227\n",
      "Epoch 187: Loss: 0.4202\n",
      "Epoch 188: Loss: 0.4188\n",
      "Epoch 189: Loss: 0.4149\n",
      "Epoch 190: Loss: 0.4153\n",
      "Epoch 191: Loss: 0.4129\n",
      "Epoch 192: Loss: 0.4091\n",
      "Epoch 193: Loss: 0.4089\n",
      "Epoch 194: Loss: 0.4073\n",
      "Epoch 195: Loss: 0.4077\n",
      "Epoch 196: Loss: 0.4047\n",
      "Epoch 197: Loss: 0.4021\n",
      "Epoch 198: Loss: 0.4019\n",
      "Epoch 199: Loss: 0.4007\n",
      "Epoch 200: Loss: 0.4006\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(data)      # Forward pass\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute loss\n",
    "    loss.backward()        # Backpropagation\n",
    "    optimizer.step()       # Update parameters\n",
    "    return loss.item()\n",
    "\n",
    "# Training for a number of epochs\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch+1}: Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize with a larger hidden layer\n",
    "model = GCN(num_features=num_features, hidden_dim=32, num_classes=2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(data)      # Forward pass\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute loss\n",
    "    loss.backward()        # Backpropagation\n",
    "    optimizer.step()       # Update parameters\n",
    "    return loss.item()\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)  # Forward pass\n",
    "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])  # Compute validation loss\n",
    "        pred = out[data.val_mask].max(1)[1]  # Get predictions\n",
    "        correct = pred.eq(data.y[data.val_mask]).sum().item()\n",
    "        accuracy = correct / data.val_mask.sum().item()  # Calculate accuracy\n",
    "    return val_loss.item(), accuracy\n",
    "\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(200):\n",
    "    train_loss = train()\n",
    "    val_loss, accuracy = validate()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve == patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5723, Test Accuracy: 0.7665\n",
      "Model saved to gnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the test function\n",
    "def test():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    out = model(data)  # Forward pass on the entire dataset\n",
    "    pred = out.argmax(dim=1)  # Get predictions\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()  # Count correct predictions\n",
    "    acc = correct / data.test_mask.sum()  # Compute accuracy\n",
    "    test_loss = criterion(out[data.test_mask], data.y[data.test_mask]).item()  # Compute test loss\n",
    "    return test_loss, acc.item()\n",
    "\n",
    "# Call the test function after training\n",
    "test_loss, test_acc = test()\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Saving the trained model to a file\n",
    "torch.save(model.state_dict(), 'gnn_model.pth')\n",
    "print('Model saved to gnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
